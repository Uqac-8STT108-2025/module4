---
title: "module4"
author: "GroupeB"
date: "`r Sys.Date()`"
output: html_document
---

```{r librairies_importation, include=FALSE}
#install.packages("ISLR2")
library(tidyverse)
library("ISLR2")
```

### Exercice1 :

* Description des hypothèses nulles :
Pour chaque variable, l'hypothèse nulle  stipule que cette qu'elle n'a aucun effet significatif sur les ventes. 
C'est à dire : 

   - TV ; H_0 : " Les dépenses en publicité télévisée n'ont pas d'effet sur les ventes."
   
   - Radio ; H_0: "Les dépenses en publicité radio n'ont pas d'effet sur les ventes."
   
   - Journaux ; H_0 : "Les dépenses en publicité dans les journaux n'ont pas d'effet sur les ventes."
   
   - Intercept ; H_0 : "La variable de référence (elle n'est pas explicitement mentionnée dans le cadre de l'exercice) 
   est non négligeable lorsque toutes les variables indépendantes sont nulles."
   
   
   
  **Interprétation des résultats** :
  
Publicité télévisée et radio : un effet significatif sur les ventes

Les valeurs p pour la TV (<0.0001) et la radio (<0.0001) sont très faibles.
Cela signifie que nous rejetons l'hypothèse nulle dans ces deux cas : il y a des preuves statistiques fortes que la publicité télévisée et la publicité radio ont un effet significatif sur les ventes.
Publicité dans les journaux : pas d'effet significatif sur les ventes

La valeur p de la publicité dans les journaux (0.8599) est très élevée.
Cela signifie que nous ne rejetons pas l'hypothèse nulle : il n'y a pas de preuve statistique que la publicité dans les journaux ait un impact sur les ventes. Autrement dit, investir dans ce type de publicité ne semble pas influencer significativement les ventes.
Intercept : significatif

La valeur p associée à l'intercept est <0.0001, donc on rejette l'hypothèse nulle.
Cela suggère qu'il existe un niveau de ventes de base même en l'absence de dépenses publicitaires.

Finalement, Les entreprises devraient privilégier la publicité télévisée et radio pour augmenter les ventes, car ces deux formes de publicité ont un impact statistiquement significatif.

### Exercice 2:

#### a) Choix de la bonne réponse : 

Nous avons $$\hat{y} = \beta _0\times + \beta _1\times GPA + \beta _2 \times QI
+ \beta _3\times Level + \beta _4\times(GPA\times QI) + \beta _5\times(GPA\times Level) + \epsilon$$


où :

- Level 1 est pour les diplômés du collège ; 

- Level 2 pour ceux du secondaire 

* Pour un diplômé du Lycée,(Level = 0), on a 

$$ \hat{y}_{Lycee} = \beta _0 +\beta _1 \times GPA + \beta _2 \times QI + \beta _4 \times GPA \times QI $$

* Pour un  diplômé du collège = universitaire : (Level = 1) on a 

$$ \hat{y}_{col} = \beta _0 +\beta _1 \times GPA + \beta _2 \times QI + \beta _3 + \beta _4 \times (GPA \times QI) + \beta _5 \times GPA $$

* On évalue maintenant la différence pour pouvoir comparer :

$$ \hat{y}_{col} - \hat{y}_{lycee} = (beta _0 +\beta _1 \times GPA + \beta _2 \times QI + \beta _3 + \beta _4 \times (GPA \times QI) + \beta _5 \times GPA) - (\beta _0 +\beta _1 \times GPA + \beta _2 \times QI + \beta _4 \times GPA \times QI)$$ 

  $$\hat{y}_{col} - \hat{y}_{lycee} = \beta _3 + \beta _5 \times GPA$$
La bonne réponse est 

iV ) Pour une valeur fixe de QI et de GPA, les diplômés universitaires gagnent en moyenne plus que les diplômés du secondaire, à condition que la GPA soit suffisamment élevée.
Mais pour que ça marche, il faut bien sûr que $\beta_3 >0$

#### b) 
Prédiction  du salaire d’un diplômé universitaire avec un QI de 110 et une moyenne cumulative de 4,0.

$$ \hat{y}_{col} = \beta _0 +\beta _1 \times GPA + \beta _2 \times QI + \beta _3 + \beta _4 \times (GPA \times QI) + \beta _5 \times GPA $$
$$ \hat{y}_{col} = \beta _0 +\beta _1 \times 4.5 + \beta _2 \times 110 + \beta _3 + \beta _4 \times (4.0 \times 110) + \beta _5 \times 4.0 $$
$$ \hat{y}_{col} = \beta _0 +4.0 \beta _1  + 110 \beta _2  + \beta _3 + 440.0 \beta _4  + 4 \beta _5  $$
Il manque les coefficients $\beta_i$ pour donner une prédiction numerique. 

#### c ) Réponse : 
Faux, ce n'est pas la valeur d'un coefficient qui détermine son interaction ou non, mais plutôt la p-value.

### Question 3 :

a ) On s'attend à ce que la RSS du modèle cubique soit plus petit que celle du modèle linéaire sur les données
d'entrainement. Par ce que le modèle cubique a plus de paramètres et capte plus de bruit, ce qui réduit significativement sa RSE.

b) regression linéaire : y_hat = a  + bX

Regression cubique : y_hat = a + bX + c X^2 + d X^3

Hypothèse nulle : c = d = 0

Si l'hypothèse nulle est rejeté (p-value significativement petit), alors le modèle cubique 
s'ajuste mieux aux donnée.

c ) (pas sur !) En toutes circonstances, la regression quadratique capte plus de bruit que la regression lineaire.
Donc la RSE est necessairement plus petite si elle est calculée sur les données test.

### Question 4 :

Question incomplete

### Question 5 :

Démontrons que dans le cas d’une régression linéaire simple, la droite des moindres carrés passe toujours par le point
moyen $G(\overline{x}, \overline{y})$.

L'équation d'une regression linéaire simple est donnée par $\hat{y} = a X + b + \epsilon$

où $$ b= \frac{\sum(x_i - \overline{x})(y_i - \overline{y})}{\sum(x_i - \overline{x})^2} $$ et 
$$b = \overline{y} - a \overline{x}$$
dès lors, $$a\overline{x} +b +\epsilon = a\overline{x} + \overline{y} - a \overline{x} \approx  \overline{y}  
 $$     cqfd.
 
### Question 6 
incomplet

## Pratique: 

### Exercice 1 

#### a) 

```{r regression-lineaire}
modele_lineaire <- lm(mpg ~ horsepower, data = Auto)
summary(modele_lineaire)
```

##### i )  
**p-value: < 2.2e-16 ** --> oui il existe une relation entre le prédicteur et la réponse ?

##### ii ) 
force de la relation entre le prédicteur et la réponse  : **R^2  0.6059** --> 60.59 % de la variabilité 
des prédictions de mpg à partir de horsepower sont expliquées par le modèle.

##### iii ) 
Signe de la relation : horsepower = -0.157845, c-a-d $\beta_ 1 <0 --> $ les deux variables évoluent en sens contraire.

##### iv ) 
mpg prévu associé à une puissance de 98 chevaux ?

```{r prediction-mpg}
predire_mpg <- function(horsepower) {
  b_0 <- 39.935861
  b_1 <- -0.157845
  return(b_0 + b_1 * horsepower)
}
predire_mpg(98)
```
La valeur prédite pour 98 chevaux est **mpg_hat = `r round(predire_mpg(98),2)`**

```{r intervalle-confiance-et-intervalle-prediction}
predict(modele_lineaire, data.frame(horsepower=98),
        interval="confidence", level=0.95)
 predict(modele_lineaire, data.frame(horsepower=98),
         interval="prediction",level=0.95)
```
La moyenne estimé du mpg pour horsepower=98 est de 24.47, avec un intervalle de confiance à 95% entre 23.97 et 24.96.

Une nouvelle voiture pour 98 chevaux pourrait avoir un mpg entre 14.80 et 34.12.
#### b)
graphique entre la réponse (mpg) et le prédicteur.

```{r graphique-modele-lineaire}
plot(Auto$horsepower, Auto$mpg, 
     main = "Relation entre MPG et Horsepower", 
     xlab = "Horsepower", 
     ylab = "MPG", 
     col = "blue")  
abline(modele_lineaire, col = "black",lwd = 3)  
legend("topright", legend = "Droite de régression", col = "black", lwd = 3)

```

 v) Quels sont les intervalles de confiance et de prédiction à 95 % associés ?

#### c) 

```{r verifications-modele-lineaire }
# Générer les diagnostics du modèle
par(mfrow = c(2, 2))  
plot(modele_lineaire)  
```
commentaires :

* La distribution des résidus a une forme quadratique,, ce qui pose problème, elle ne doit pas avoir 
une répartition perceptible

* Le diagramme Quantiles-quantile a une forme lineaire --> Les résidus suivent une distribution normale



### Question 2 
Cette question implique l’utilisation d’une régression linéaire multiple sur l’ensemble de données Auto.

#### a)
Produisons une matrice de nuages de points qui inclut toutes les variables de l’ensemble de données.

```{r nuage_points}
library(ISLR)
data(Auto) 
pairs(Auto, main = "Matrice de nuages de points pour Auto")
```

#### b)
Calculons la matrice des corrélations entre les variables à l’aide de la fonction cor(). nous devions exclure la variable name qui est qualitative.

```{r matrice de correlation}
data(Auto)
#Extrait la variable "name"
Auto_numeric <- Auto[, -which(names(Auto) == "name")]

#Calculons la matrice des corrélations
cor_matrix <- cor(Auto_numeric)

print(round(cor_matrix, 2))
```
#### c) 
Utilisons la fonction lm() pour effectuer une régression linéaire multiple avec mpg comme réponse et toutes les autres variables sauf le nom comme prédicteurs. Utilisez la fonction summary() pour imprimer les résultats. Commentons le résultat. Par exemple:

 i- Existe-t-il une relation entre les prédicteurs et la réponse ?

**p-value < 2.2e-16 pour le test F, ce qui indique confirment qu'il existe une forte relation entre les prédicteurs choisis et mpg, permettant de modéliser efficacement la consommation de carburant des véhicules présents dans le jeu de données Auto.

```{r relation_predicteurs}
data(Auto)
modele_multiple <- lm(mpg ~ . - name, data = Auto)
summary(modele_multiple)
```

ii- Quels prédicteurs semblent avoir une relation statistiquement significative avec la réponse ?

Dans le modèle de régression multiple de (i), on vas sélectionner les prédicteurs qui présentent des p‑values très faibles (inférieures à 0,05):
horsepower:p = 0,21
weight: p = 2e‑16
year:p < 2e‑16
origin:p= 0,013

iii-Que suggère le coefficient de la variable « année » 

Le coefficient de la variable «année» est positif d'environ 0,75, ce qui suggère qu'en moyenne, chaque année additionnelle est associée à une augmentation d'environ 0,75 mpg. Autrement dit, les véhicules plus récents tendent à afficher une meilleure efficacité énergétique.



#### d)
Utilisons la fonction plot() pour produire des tracés de diagnostic de l’ajustement de régression linéaire. Commentez tous les problèmes que vous voyez avec l’ajustement. 

Avec le modèle_simple

```{r modele_simple}
#plaçons les graphe 2 a 2
par(mfrow = c(2, 2))
#graphe
plot(modele_multiple)
```
* Les tracés résiduels suggèrent-ils des valeurs aberrantes inhabituellement importantes ? 

D'après les tracés de diagnostic d'ajustement générés notamment le graphique "Residuals vs Fitted" et le "Residuals vs Leverage", aucun point ne se détache de manière excessive.
les résidus semblent globalement aléatoires et bien répartis, sans valeurs aberrantes inhabituelles, et le graphique d'effet de levier ne signale pas d'observations avec un effet de levier exceptionnellement élevé.

* Le graphique de l’effet de levier identifie-t-il des observations présentant un effet de levier inhabituellement élevé ?

le graphique de l'effet de levier (résidus vs levier) ne révèle aucune observation se situant en dehors des limites habituelles, Aucun point ne présente un effet de levier anormalement.

#### e)
Utilisez les symboles * et : pour ajuster les modèles de régression linéaire avec des effets d’interaction. Certaines interactions semblent-elles statistiquement significatives ?

Modèle avec interaction entre horsepower et year.

le coefficient d'interaction est négatif et statistiquement significatif car p-value < 0.05, cela suggère que l'effet négatif de horsepower sur mpg devient plus prononcé pour des véhicules plus récents.

le modele avec *

```{r régression_linéaire}
modele_interaction <- lm(mpg ~ horsepower * year, data = Auto)
summary(modele_interaction)
```
le modele avec :

```{r regression_linéaire }
modele_interaction2 <- lm(mpg ~ horsepower : year, data = Auto)
summary(modele_interaction2)
```
#### f) 
Essayons quelques transformations différentes des variables, telles que log(X), sqrt(X), X*2 Commentons vos découvertes.

Modèle original : Le modèle utilisant horsepower en l'état fournit une relation significative négative avec mpg.

```{r modèle_original}
#Modèle avec la variable originale
model_orig <- lm(mpg ~ horsepower, data = Auto)
summary(model_orig)
par(mfrow = c(2, 2))
plot(model_orig, main = "Modèle original")
```

Transformation log :L'utilisation de log(horsepower) linéarise la relation qui était autrement curviligne dans le modèle original et montre une de l'ajustement.

```{r modèle_log(X)}
# Modèle avec log(horsepower)
model_log <- lm(mpg ~ log(horsepower), data = Auto)
summary(model_log)
par(mfrow = c(2, 2))
plot(model_log, main = "Modèle log(horsepower)")
```


Transformation racine carrée :Le modèle avec sqrt(horsepower) réduire l'impact des valeurs élevées, améliore la linéarité de la relation ce qui améliore la répartition des résidus.

```{r modèle_sqrt}
# Modèle avec racine carrée de horsepower
model_sqrt <- lm(mpg ~ sqrt(horsepower), data = Auto)
summary(model_sqrt)
par(mfrow = c(2, 2))
plot(model_sqrt, main = "Modèle sqrt(horsepower)")
```

Transformation au carré :cette transformation ne semble généralement pas apporter une signification, et le terme peut même rendre l'interprétation plus complexe.

```{r modèle_carrée}
# Modèle avec horsepower au carré
model_sq <- lm(mpg ~ I(horsepower^2), data = Auto)
summary(model_sq)
par(mfrow = c(2, 2))
plot(model_sq, main = "Modèle horsepower^2")
```
### Question 3 

#### a)
Ajustons un modèle de régression multiple pour prédire les « Ventes » en utilisant « Prix », « Urbain » et « États-Unis ».

Nous allons ajuster le modèle de regression multiple utiliser la haut

```{r regression_multiple}
data(Carseats)
#Ajustons le modele utiliser la haut
modele_carseats <- lm(Sales ~ Price + Urban + US, data = Carseats)
summary(modele_carseats)
```

#### b)
Fournissons une interprétation de chaque coefficient du modèle. Soyons prudent : certaines variables du modèle sont qualitatives !

Intercept: 13.04 Ce coefficient représente la valeur prédite des ventes (Sales) pour le groupe de référence,les variables qualitatives sont codées en variables indicatrices. 
l'interception correspond aux ventes moyennes prédites pour un magasin situé dans une zone urbaine (Urban = Yes) hors des États-Unis (US = No) avec un Prix égal à 0.

Price: -0.054459 Pour chaque augmentation d'une unité du prix, en maintenant constantes toutes les autres variables, les ventes prédites diminuent en moyenne de 0.054459 unité. Cela indique que le prix a un effet négatif sur les ventes.

UrbanNo : -0.021916 Cette variable est un indicateur qui vaut 1 lorsque le magasin n'est pas situé en zone urbaine (Urban = No) et 0 lorsque le magasin est en zone urbaine (Urban = Yes). 
Le coefficient de -0.021916 suggère que, toutes choses égales par ailleurs, les magasins situés hors zone urbaine affichent des ventes prédites inférieures de 0.021916 unité par rapport aux magasins urbains. Cependant, cette différence n'est pas statistiquement significative car p-value ≈ 0,936.

USYes : 1.200573 Cette variable indicatrice vaut 1 pour les magasins situés aux États‑Unis (US = Yes) et 0 pour ceux hors des États‑Unis (US = Ne).
Le coefficient de 1.200573 indique qu'en moyenne, les magasins situés aux États-Unis affichent des ventes prédites supérieures de 1.200573 unité par rapport aux magasins non américains, toutes choses étant égales par ailleurs. Cet effet est très significatif car p-value < 2e‑1).

```{r modele_carseats}
modele_carseats <- lm(Sales ~ Price + Urban + US, data = Carseats)
summary(modele_carseats)
```
#### c)
Écrivons le modèle sous forme d’équation, en prenant soin de gérer correctement les variables qualitatives.

Sales= coef(Intercept) + coef(price)*price + coef(UrbanNo)*(Urban=no) + coef(USYes)*(US= yes).

 En application:
 
Sales = 13.04 +(-0.054459)*price + (-0.21916)*(Urban=No) + (1.200573)*(US=Yes)

NB:(Urban=No) vaut 1 si le magasin n'est pas en zone urbaine  sinon elle vaut 0 si (Urban = Yes).
-(US=YES) vaut 1 si le magasin est situé aux États-Unis  sinon elle vaut 0 si (US = NO).

#### d)
Pour lequel des prédicteurs pouvez-vous rejeter l’hypothèse nulle H₀ : Bⱼ = 0 ?

D'après le résumé du modèle sur l'ensemble de données Carseats price et us ont les plus faibles p-value, donc nous pouvons rejeter l'hypothèse nulle H₀ : Bⱼ = 0 pour les variables Price et US.

#### e)
Sur la base de notre réponse à la question précédente, ajustons un modèle plus petit qui utilise uniquement les prédicteurs pour lesquels il existe des preuves d’association avec le résultat.

Ajustons le modèle réduit en ne retenant que les prédicteurs significatifs (Price et US) :

```{r predicteurs_association}
data(Carseats)
modele_carseats_small <- lm(Sales ~ Price + US, data = Carseats)
summary(modele_carseats_small)
```
#### f)
Dans quelle mesure les modèles (a) et (e) s’ajustent-ils aux données ?

Les deux modèles s’ajustent globalement bien aux données. Par exemple, dans le modèle (a) complet qui inclut Price, Urban et US.Meme en ajustant un modèle réduit (modèle(e)) ne retenant que Price et US (car Urban n'est pas significatif), on obtient des statistiques très proches

En gros la capacité prédictive du modèle est quasiment la même que l'on utilise ou non la variable Urban, ce qui confirme que seuls Price et US apportent une information significative pour expliquer Sales.

#### g) 
À l’aide du modèle de (e), obtenons des intervalles de confiance de 95 % pour le(s) coefficient(s).

```{r intervall_confiance}
data(Carseats)
modele_carseats_small <- lm(Sales ~ Price + US, data = Carseats)
confint(modele_carseats_small, level = 0.95)
```

#### h) 
Existe-t-il des preuves de valeurs aberrantes ou d’observations à effet de levier élevé dans le modèle de (e) ?

Le graphique "Residuals vs Fitted" montre que les résidus sont bien répartis sans points extrêmes.
Le "Normal QQ Plot" indique que les résidus suivent globalement une distribution normale.

Le tracé "Residuals vs Leverage" ne met pas en évidence d'observations avec un effet de levier

Dans l'ensemble, ces diagnostics ne révèlent pas de valeurs aberrantes ni d'observations avec un effet de levier excessif dans le modèle(e).

```{r graphe observation levier}
data(Carseats)
modele_carseats_small <- lm(Sales ~ Price + US, data = Carseats)
par(mfrow = c(2, 2))
plot(modele_carseats_small)
```

### Question 4

```{r generation-alleatoire-predicteur}
  set.seed(1)
  x <- rnorm(100)
  y <- 2 * x + rnorm(100)
```

#### a) 
Regression lineaire sans intercept de y sur x

```{r regression-sans-intercept-de-y-sur-x}
modele_lin_yx <- lm(y~x+0)
summary(modele_lin_yx)

```
* Estimation du coefficient :  **beta1_hat = 1.9939**,

*erreur type : **std = 0.1065**,

* statistique t : **t = 18.73**, 

*p-value : **p < 2e-16  **

* Commentaire : Il existe une correlation positive entre x et y. La p-value est extremement faible, ce qui signifie que nous rejetons l'hypothese nulle (beta1 = 0) ==> il y a une forte correlation lineaire entre ces deux variables, et 
le coefficient estimé est significativement différent de 0.

#### b)
Regression lineaire sans intercept de x sur y

```{r regression-sans-intercept-de-x-sur-y}
modele_lin_xy <- lm(x~y+0)
summary(modele_lin_xy)
```
* Estimation du coefficient : **beta1_hat = 0.39111 **

* erreur type : **std = 0.02089**

* statistique t : **t = 18.73**

* p-value : **p <2e-16**

* Commentaire : Il existe une correlation positive entre x et y. La p-value est extremement faible, ce qui signifie que nous rejetons l'hypothese nulle (beta_1 = 0) ==> il y a une forte correlation lineaire entre ces deux variables, et 
le coefficient estimé est significativement différent de 0.

#### c) 
Les deux relations ont la même statistique t et la meme p-value. De plus, le produit des deux 
coefficient est égal à R^2

R^2= 0.7798 et   beta1_xy * beta1-yx  = `r  round(modele_lin_xy$coefficients * modele_lin_yx$coefficients,4)`

#### d) 
Question manquante

#### e)
Question manquante

#### f)
Dans R, montrez que lorsque la régression est effectuée avec un intercept la statistique t pour 
est la même pour la régression de y sur x que pour la régression de x sur y.

* Regression linéaire de y sur x avec intercept : 

```{r regression-avec-intercept-de-y-sur-x, include=FALSE}
modele_lin_yx_intercep <- lm(y~x)
model_summary_yx <- summary(modele_lin_yx_intercep)
t_stat_yx <- model_summary_yx$coefficients["x", "t value"]
cat('la statistique du test est ' , t_stat_yx )
```
La statistique du test est : **t= `r t_stat_yx`**

** Regression linéaire de x sur y avec intercept : 

```{r regression-avec-intercept-de-x-sur-y, include= FALSE}
modele_lin_xy_intercep <- lm(x~y)
model_summary_xy <- summary(modele_lin_xy_intercep)
t_stat_xy <- model_summary_yx$coefficients["x", "t value"]
cat('la statistique du test est ' , t_stat_xy )

```
La statistique du test est : **t= `r t_stat_xy`**


### Question 5

#### a) 
Dans quelles circonstances l’estimation du coefficient pour la régression de y sur x est-elle la même que l’estimation du coefficient pour la régression de x sur  y ?

* coefficient de correlation de y sur x : $$ b_{yx}1 = \frac{cov(x,y)}{V(x)}$$

* coefficient de correlation de x sur y : $$ b_{xy}1 = \frac{cov(x,y)}{V(y)}$$

* $$ b_{yx}1 = b_{xy}1 \iff V(x) = V(y)  $$

l'estimation du coefficient pour la régression de Y sur X est la même que celle de la régression de X sur Y lorsque les variances de x et y sont égales.

#### b)
Générons un exemple dans R avec des observations dans lequel l’estimation du coefficient pour la régression de y sur x est différente de l’estimation du coefficient pour la régression de x sur y .

```{r difference-coefficient-regression}
#Exemple : Y = 2 * X (donc c = 2)
set.seed(123)
x <- rnorm(100)
y <- 2 * x

#La régression sans intercept de Y sur X
model_y_on_x <- lm(y ~ 0 + x)
coef_y_on_x <- coef(model_y_on_x)

#La régression sans intercept de X sur Y
model_x_on_y <- lm(x ~ 0 + y)
coef_x_on_y <- coef(model_x_on_y)

cat("Coefficient pour la régression de Y sur X :", round(coef_y_on_x, 3), "\n")
cat("Coefficient pour la régression de X sur Y :", round(coef_x_on_y, 3), "\n")
```

#### c) 
Générons un exemple dans R avec  des observations dans lequel l’estimation du coefficient pour la régression de y  sur x est la même que l’estimation du coefficient pour la régression de x sur y.

```{r exemple estimation du coefficient regression ou X,Y}
set.seed(123)
x <- rnorm(100)
y <- x  #Y est exactement égal à X

#La régression sans intercept de Y sur X
modele_Y_on_X <- lm(y ~ 0 + x)
coef_Y_on_X <- coef(modele_Y_on_X)

#La régression sans intercept de X sur Y
modele_X_on_Y <- lm(x ~ 0 + y)
coef_X_on_Y <- coef(modele_X_on_Y)

cat("Coefficient pour la régression de Y sur X :", round(coef_Y_on_X, 3), "\n")
cat("Coefficient pour la régression de X sur Y :", round(coef_X_on_Y, 3), "\n")
```
### Question 6

#### a)
Pour créer un vecteur x de 100 observations tirées d'une distribution normale standard (N(0,1)) en assurant la reproductibilité, on vas utiliser :

```{r vecteur x avec 100 observation}
set.seed(1)
x <- rnorm(100)
```

#### b)
Pour créer un vecteur eps de 100 observations issues d'une distribution normale avec une moyenne nulle et une variance de 0,25 (donc un écart type de 0,5), ont vas utiliser :

```{r vecteur eps avec 100 observation }
eps <- rnorm(100, mean = 0, sd = 0.5)
```

#### c)
En utilisant x et eps, générons un vecteur y selon le modèle 
Quelle est la longueur du vecteur y ? 
la longueur est 100

Quelles sont les valeurs de et dans ce modèle linéaire ?
La valeur est 

```{r vecteur y}
y <- 2 + 2 * x + eps
length(y)
```


#### d)

Le graphique montre une relation linéaire positive entre x et y, ce qui est conforme au modèle générateur x=2+2𝑥+𝜖de plus aucun comportement non linéaire ni valeurs aberrantes marquées ne semblent apparaître, ce qui confirme que la relation entre x et y est bien linéaire
```{r graphe rtion entre X et Y}
plot(x, y, main = "Nuage de points : x vs y", xlab = "x", ylab = "y", pch = 19, col = "blue")
```

#### e)
Le coefficient de l'interception est estimé à environ 2,03 et celui de x à environ 1,98, ce qui est très proche des valeurs théoriques 2 et 2.
Les p‑values extrêmement faibles (p_value< 2e-16) indiquent que les deux coefficients sont très significatifs.
Le Multiple R carrée (environ 0.941) montre que le modèle explique une grande partie de la variance de y.
Ces résultats confirment que le modèle amélioré par moindres carrés reproduit correctement le modèle générateur x=2+2 x+ϵ.

```{r modèle lineaire des moindres carrée }
set.seed(1)
x <- rnorm(100)
eps <- rnorm(100, mean = 0, sd = 0.5) 
y <- 2 + 2 * x + eps
modele <- lm(y ~ x)
summary(modele)
```
#### f)
le nuage de points, ajoute la ligne de régression obtenue par les moindres carrés (modèle ajusté) et la vraie ligne de régression

```{r Nuage de points avec ligne des moindres carrés et vraie régression}
set.seed(1)
x <- rnorm(100)
eps <- rnorm(100, mean = 0, sd = 0.5)  
y <- 2 + 2 * x + eps
plot(x, y, main = "Nuage de points avec ligne des moindres carrés et vraie régression",
     xlab = "x", ylab = "y", pch = 19, col = "gray")
modele <- lm(y ~ x)
abline(modele, col = "blue", lwd = 2)
abline(a = 2, b = 2, col = "red", lwd = 2, lty = 2)

#on ajout d'une légende
legend("topleft", legend = c("Ligne des moindres carrés", "Régression de la population"),
       col = c("blue", "red"), lty = c(1, 2), lwd = 2)
```
#### g)

Explication:
le modèle générateur est x=2+2 x+ϵ(donc linéaire) et l'ajout du terme x² n'a pas apporter d'amélioration significative.
Or dans le modèle polynomial, le coefficient associé à I(x²) permet d'évaluer l'effet quadratique.
Si ce coefficient n'est pas statistiquement significatif (p-value > 0.05) et si le test d'ANOVA (comparaison entre le modèle linéaire et le modèle polynomial) ne montre pas une significative (p-value > 0.05), cela indique qu'il n'existe pas de preuves que le terme quadratique améliore l'ajustement du modèle.

```{r modèle de régression polynomiale}
set.seed(1)
x <- rnorm(100)
eps <- rnorm(100, mean = 0, sd = 0.5)
y <- 2 + 2 * x + eps
model_linear <- lm(y ~ x)
model_poly <- lm(y ~ x + I(x^2))

summary(model_linear)
summary(model_poly)

anova(model_linear, model_poly)
```
#### h)
En diminuant la variance du terme d'erreur, on obtient des estimations plus précises et un meilleur ajustement global du modèle aux données, tout en confirmant que le modèle linéaire simple est bien adapté à la structure de la relation simulée.

```{r processus de génération de données pour diminuer le bruit}
#(a) Générons des données avec moins de bruit
set.seed(1)
x <- rnorm(100)                        
eps <- rnorm(100, mean = 0, sd = 0.25)   
y <- 2 + 2 * x + eps                   

#(b) Vérification de la longueur y=100
length(y)  

#(c) Nuage de points
plot(x, y, main = "Nuage de points : x vs y (moins de bruit)",
     xlab = "x", ylab = "y", pch = 19, col = "blue")

#(d) Ajustement d'un modèle linéaire simple
model_linear <- lm(y ~ x)
summary(model_linear)

#(e) Ajoutons de la droite des moindres carrés et de la vraie ligne de régression
plot(x, y, main = "x vs y : Moindres carrés vs Vraie régression (moins de bruit)",
     xlab = "x", ylab = "y", pch = 19, col = "gray")
abline(model_linear, col = "blue", lwd = 2)
abline(a = 2, b = 2, col = "red", lwd = 2, lty = 2)
legend("topleft", legend = c("Modèle ajusté", "Modèle vrai"),
       col = c("blue", "red"), lty = c(1, 2), lwd = 2)

#(f) Ajustement d'un modèle de régression polynomiale (avec x et x^2) et comparaison
model_poly <- lm(y ~ x + I(x^2))
summary(model_poly)
anova(model_linear, model_poly)
```
#### i)

Le fait d'augmenter la variance du terme d'erreur rend le modèle moins précis (erreurs standards plus élevées, R² plus faible), et le nuage de points montre une plus grande dispersion. Cependant, la relation linéaire reste clairement visible, et le modèle amélioré par moindres carrés reste cohérent avec le modèle théorique x=2+2 x+ϵ.

```{r  génère des données avec plus de bruit}
#(a) Générons des données avec plus de bruit
set.seed(1)
x <- rnorm(100)                       
eps <- rnorm(100, mean = 0, sd = 1)      
y <- 2 + 2 * x + eps                  

#(b) Vérification de la longueur du vecteur y=100
length(y)  

#(c) Nuage de points entre x et y
plot(x, y, main = "Nuage de points : x vs y (plus de bruit)",
     xlab = "x", ylab = "y", pch = 19, col = "blue")

#(d) Ajustement du modèle linéaire simple et résumé
model_linear <- lm(y ~ x)
summary(model_linear)

#(e) Tracons les lignes : 
plot(x, y, main = "x vs y : Modèle ajusté vs fonction vraie (plus de bruit)",
     xlab = "x", ylab = "y", pch = 19, col = "gray")
abline(model_linear, col = "blue", lwd = 2)
abline(a = 2, b = 2, col = "red", lwd = 2, lty = 2) 
legend("topleft", legend = c("Modèle ajusté", "Modèle vrai"),
       col = c("blue", "red"), lty = c(1, 2), lwd = 2)

#(f) Ajustement du modèle de régression polynomiale (incluant x et x^2)
model_poly <- lm(y ~ x + I(x^2))
summary(model_poly)

#Comparaison entre le modèle linéaire simple et le modèle polynomial
anova(model_linear, model_poly)
```

#### j)

Explication:
Dans le jeu de données avec moins de bruit , la dispersion autour de la droite de régression est plus faible. Par conséquent, les intervalles de confiance pour l'interception et le coefficient de x sont très étroits, reflétant une incertitude moindre dans les estimations.

Dans le jeu de données avec plus de bruit , la variabilité des observations autour de la droite est plus élevée, ce qui conduit à des erreurs standards plus grandes et à des intervalles de confiance plus larges. Les intervalles sont ainsi plus étendus, traduisant une incertitude accumulée dans l'estimation des coefficients.

Dans le jeu de données original , les intervalles se situent entre ceux obtenus avec moins de bruit et ceux obtenus avec plus de bruit, ce qui est cohérent avec le fait que la variance du terme d'erreur influe directement sur la précision des estimations.

Ces observations montrent clairement comment la quantité de bruit (variance de l'erreur) affecte la précision (et donc la largeur des intervalles de confiance) des coefficients estimés dans un modèle de régression linéaire.

```{r  intervalles de confiance}
set.seed(1)
#Le Jeu de données original
x <- rnorm(100)
eps_original <- rnorm(100, mean = 0, sd = 0.5)
y_original <- 2 + 2 * x + eps_original
model_original <- lm(y_original ~ x)
ci_original <- confint(model_original, level = 0.95)
print("Intervalle de confiance (jeu de données original) :")
print(round(ci_original, 3))

#Le Jeu de données avec plus de bruit
eps_bruit <- rnorm(100, mean = 0, sd = 1)
y_bruit <- 2 + 2 * x + eps_bruit
model_bruit <- lm(y_bruit ~ x)
ci_bruit <- confint(model_bruit, level = 0.95)
print("Intervalle de confiance (jeu de données avec plus de bruit) :")
print(round(ci_bruit, 3))

# Jeu de données avec moins de bruit (ε ~ N(0, 0.25²))
eps_mbruit <- rnorm(100, mean = 0, sd = 0.25)
y_mbruit <- 2 + 2 * x + eps_mbruit
model_mbruit <- lm(y_mbruit ~ x)
ci_mbruit <- confint(model_mbruit, level = 0.95)
print("Intervalle de confiance (jeu de données avec moins de bruit) :")
print(round(ci_mbruit, 3))
```
### Question 7

```{r question-7a}
set.seed(1)
x1 <-runif(100)
x2 <- 0.5 * x1 + rnorm(100) / 10
y <- 2 + 2 * x1 + 0.3 * x2 + rnorm(100)
```

#### a) 

* forme du modele linéaire : y = 2 + 2x1 + 0.3 x2

* coefficients : intercept = 2, coeff de x1 = 2, coeff de x2 = 0.3

#### b)
correlation entre x1 et x2

* La correlation entre x1 et x2 est **corr(x,y) = `r round(cor(x1,x2),2)`**

```{r nuage-des-points-x1-x2}
data <- data.frame(x1, x2)
ggplot(data, aes(x=x1, y=x2)) +
  geom_point(color="blue") +
  labs(title = "Nuage de points entre x1 et x2",
       x = " ",
       y = " ")

```
#### c)
```{r regression-moindre-carre}
modele_lmx1_x2 <- lm(y ~ x1 + x2)
summary(modele_lmx1_x2)

```
L'intercept et le coefficient de x1 sont très petits, ce qui signifie que ces valeurs sont significatives. En revanche, 
la p-value de x2 est grand ==> x2 n'a pas d'effet significatif sur y dans notre modèle.

#### d)

```{r prediction-y-en-fonction-de-x1}
modele_lm_x1 <- lm(y ~ x1)
summary(modele_lm_x1)

```
Puisque la p-value associée à x1 est extrêmement faible, nous rejetons l'hypothèse nulle et concluons que 
x1 a un effet statistiquement significatif sur y

#### e) 

```{r prediction-y-en-fonction-de-x2}
modele_lm_x2 <- lm(y ~ x2)
summary(modele_lm_x2)

```
Puisque la p-value associée à x2 est extrêmement faible, nous rejetons l'hypothèse nulle et concluons que 
x2 a un effet statistiquement significatif sur y.

#### f) 

Cette contradiction est du au fait de la multicolinéarité entre x1 et x2. 

Seul, x2 a un effet significatif, mais combiné avec x2, son effet est négligeable par ce qu'il lui même 
expliqué par  x1 via l'expression **x2 <- 0.5 * x1 + rnorm(100) / 10**

#### g)

```{r reajustement}
x1<-c(x1,0.1)
x2<-c(x2,0.8)
y<-c(y,6)

# Modèle linéaire par moindre carrée
modele_lm_x1_x2<-lm(y~x1+x2)
summary(modele_lm_x1_x2)
#prédiction de y en fonction de x1
modele_lm_x1<-lm(y~x1)
summary(modele_lm_x1)

#prediction de y en fonction de x2
modele_lm_x2<-lm(y~x2)
summary(modele_lm_x2)
plot(y,x1,lwd=3, col="red")


```

Avec cette réajustement, pour le modèle linéaire par moindre carrée,la p-value de x1 de 0.36 est supérieure à 0.05 et la p-value de x2 de 0.006 est très petite à 0.05 donc on ne rejette pas l'hypothèse nulle qui est $\ H_0:\beta_1\ =0$, il n'y a pas de relation significative entre x1 et la variable réponse y. Par contre, la p-value de x2 de 0.006 est très petite à 0.05, on rejette $\ H_0:\beta_2\ =0$ on peut conclure qu' il y a une relation significative entre x2 et y.

Pour la prédiction de y en fonction de x1: la p-value =4.30e-05 est très petite à 0.05 donc on rejette $\ H_0:\beta_1\ =0$, on peut conclure qu'il y a une relation siginificative entre x1 et y.

Pour la prédiction de y en fonction de x2 :la p-value de x2 de 1.253e-06 est très petite à 0.05 donc on rejette l'hypothèse nulle qui est  $\beta_2\ =0$, il y a une relation significative entre x2 et la variable réponse y.
```{r aberrante-levier-x1-x2}
#Pour le modèle linéaire par moindre carrée

plot(modele_lm_x1_x2)
#Point à fort effet de levier 
which.max(hatvalues(modele_lm_x1_x2))
```
 L'observation est aberrante avec les points 21,55 et 82 qui sont éloignés des valeurs prédites par le modèle.Oui, on a un point à fort effet de levier qui est de 101.

```{r aberrante-levier-x1}
#Pour la prédiction de y en fonction de x1
plot(modele_lm_x1) 
#Point à fort effet de levier
which.max(hatvalues(modele_lm_x1))

```
On a une valeur aberrante pour le point 101.On a un point à fort effet de levier qui est 27.
```{r aberrante-levier-x2}
#Pour la prédiction de y en fonction de x2
plot(modele_lm_x2) 

#Point à fort effet de levier
which.max(hatvalues(modele_lm_x2))

```
On a une valeur aberrante pour le point 21,55,82.On a un point à fort effet de levier qui est 101. 
En conclusion: l'observation  ayant un effet de levier élevé a une valeur inhabituelle pour x2.

### Question 8 

#### a)
```{r regression-lineaire-boston}
View(Boston)

#prédire le taux de criminalité en fonction de la proportion de terrains résidentiels 
lm_zn<-lm(crim~zn,data=Boston)
summary(lm_zn)

#prédire le taux de criminalité en fonction de la proportion d'hectares consacrés à d'autres activités que le commerce
lm_indus<-lm(crim~indus,data=Boston)
summary(lm_indus)

#prédire le taux de criminalité en fonction de la variable muette 
lm_chas<-lm(crim~chas,data=Boston)
summary(lm_chas)

#prédire le taux de criminalité en fonction de la concentration d'oxydes d'azote
lm_nox<-lm(crim~nox,data=Boston)
summary(lm_nox)

#prédire le taux de criminalité en fonction du nombre moyen de pièces par logement
lm_rm<-lm(crim~rm,data=Boston)
summary(lm_rm)

#prédire le taux de criminalité en fonction de la proportion de logements occupés par leur propriétaire
lm_age<-lm(crim~age,data=Boston)
summary(lm_age)

#prédire le taux de criminalité en fonction de l'indice d'accessibilité aux autoroutes radiales
lm_rad<-lm(crim~rad,data=Boston)
summary(lm_rad)

#prédire le taux de criminalité en fonction du taux d'imposition
lm_tax<-lm(crim~tax,data=Boston)
summary(lm_tax)

#prédire le taux de criminalité en fonction du taux d'encadrement par ville 
lm_ptratio<-lm(crim~ptratio,data=Boston)
summary(lm_ptratio)

#prédire le taux de criminalité en fonction du statut inférieur de la population
lm_lstat<-lm(crim~lstat,data=Boston)
summary(lm_lstat)

#prédire le taux de criminalité en fonction de la valeur médiane des maisons occupées par leur propriétaire
lm_medv<-lm(crim~medv,data=Boston)
summary(lm_medv)

#Graphique 
plot(crim~zn,data=Boston)
abline(lm_zn, col="red")

plot(crim~indus,data=Boston)
abline(lm_indus, col="red")

plot(crim~nox,data=Boston)
abline(lm_nox, col="red")

plot(crim~rm,data=Boston)
abline(lm_rm, col="red")

plot(crim~age,data=Boston)
abline(lm_age, col="red")

plot(crim~rad,data=Boston)
abline(lm_rad, col="red") 

plot(crim~tax,data=Boston)
abline(lm_tax, col="red") 

plot(crim~ptratio,data=Boston)
abline(lm_ptratio, col="red")

plot(crim~lstat,data=Boston)
abline(lm_lstat, col="red")

plot(crim~medv,data=Boston)
abline(lm_medv, col="red")
```




Pour le prédicteur Zn, on a un p-value de 5.51e-06 qui est plus petit que 0.05. On rejette l'hypothèse nulle $\ H_0: \beta_1\ = 0$. Il y a une relation significative entre la proportion de terrains résidentiels(zn) et le taux de criminalité (crim).

Pour le prédicteur Indus, on a une p-value (2e-16) qui est plus petit que 0.05. On rejette l'hypothèse nulle $\ H_0: \beta_2\ = 0$. Il y a une relation significative entre la proportion d'hectares consacrés à d'autres activités (indus) et le taux de criminalité.

Pour le prédicteur Chas, on a une p-value (0.209) qui est plus grand que 0.05. On ne rejette pasl'hypothèse nulle $\ H_0: \beta_3\ = 0$. Il n'y a pas de relation significative entre la variable muette Charles River(Chas) (c'est à dire que le territoire soit boirdé par la rivière ou non) et le taux de criminalité.

Pour le prédicteur Nox, on a une p-value (2e-16) qui est plus petit que 0.05. On  rejette l'hypothèse nulle $\ H_0: \beta_4\ = 0$. Il y a  une relation significative entre la concentration d'oxydes d'azote(nox)  et le taux de criminalité.

Pour le prédicteur Rm, on a une p-value (6.35e-07) qui est plus petit que 0.05. On  rejette l'hypothèse nulle $\ H_0: \beta_5\ = 0$. Il y a  une relation significative entre le nombre moyen de pièces par logement(rm)  et le taux de criminalité.

Pour le prédicteur Age, on a une p-value (2.85e-16) qui est plus petit que 0.05. On  rejette l'hypothèse nulle $\ H_0: \beta_6\ = 0$. Il y a  une relation significative entre la proportion de logements occupés par leur propriétaire (age)  et le taux de criminalité.

Pour le prédicteur Rad, on a une p-value (2e-16) qui est plus petit que 0.05. On  rejette l'hypothèse nulle $\ H_0: \beta_7\ = 0$. Il y a  une relation significative entre l'indice d'accessibilité aux autoroutes radiales  (rad)  et le taux de criminalité.

Pour le prédicteur Tax, on a une p-value (2e-16) qui est plus petit que 0.05. On  rejette l'hypothèse nulle $\ H_0: \beta_8\ = 0$. Il y a  une relation significative entre le taux d'imposition  (tax)  et le taux de criminalité.

Pour le prédicteur Ptratio, on a une p-value (2.94e-11) qui est plus petit que 0.05. On  rejette l'hypothèse nulle $\ H_0: \beta_9\ = 0$. Il y a  une relation significative entre le taux d'encadrement  (ptratio)  et le taux de criminalité.

Pour le prédicteur Lstat, on a une p-value (2e-16) qui est plus petit que 0.05. On  rejette l'hypothèse nulle H0: Beta_10=0 . Il y a  une relation significative entre le statut inférieur de la population  (lstat)  et le taux de criminalité.

Pour le prédicteur Medv, on a une p-value (2e-16) qui est plus petit que 0.05. On  rejette l'hypothèse nulle H0: Beta_11=0 . Il y a  une relation significative entre la valeur médiane des maisons occupées par leur propriétaire (medv)  et le taux de criminalité.

En résumé, il existe des relations significatives entre chaque prédicteur Zn, Indus, Nox, rm, age, ,rad,tax,ptratio,lstat et medv  et la réponse.

Les graphiques nous ont permis pour certains modèles de confirmer les relations entre le prédicteur et la réponse toutefois pour le prédicteur indice d'accessibilité aux autoroutes radiales (rad), la relation n'est pas bien representé sur le graphique.
#### b) 

```{r boston}
head(Boston)
modele_lin <- lm(crim ~., data = Boston)
summary(modele_lin)
```
* On va rejeter l'hypothese nulle pour les variables 
'zn', 'indus', 'chas', 'nox', 'age', 'rm', 'tax', 'ptratio', et 'lstat'

### c)

Pour le modèle de régression simple, il existe une relation significative entre tous les prédicteurs et la réponse sauf pour le prédicteur chas. Hors pour le modèle de régression multiple, le prédicteur chas a une relation significative avec la réponse . Ce qui peut être expliquer par le fait que le prédicteur chas (variable muette Charles River) explique les autres variables prédicteurs.
```{r coefficient-univariée-multiple }


```

### d) 

```{r association-non-lineaire}
# Modèle pour le prédicteur Zn (a verifier)

modele_zn<-lm(crim~poly(zn,3),data=Boston)
summary(modele_zn)

#modèle pour le prédicteur indus 
modele_indus<-lm(crim~poly(indus,3),data=Boston)
summary(modele_indus)

#modèle pour le prédicteur nox 
modele_nox<-lm(crim~poly(nox,3),data=Boston)
summary(modele_nox)

#modèle pour le prédicteur rm (a verifier)
modele_rm<-lm(crim~poly(rm,3),data=Boston)
summary(modele_rm)

#modèle pour le prédicteur age
modele_age<-lm(crim~poly(age,3),data=Boston)
summary(modele_age)

#modèle pour le prédicteur rad
modele_rad<-lm(crim~poly(rad,3),data=Boston)
summary(modele_rad)

#modèle pour le prédicteur tax
modele_tax<-lm(crim~poly(tax,3),data=Boston)
summary(modele_tax)
#modèle pour le prédicteur ptratio
modele_ptratio<-lm(crim~poly(ptratio,3),data=Boston)
summary(modele_ptratio)
#modèle pour le prédicteur lstat
modele_lstat<-lm(crim~poly(lstat,3),data=Boston)
summary(modele_lstat)
#modèle pour le prédicteur medv
modele_medv<-lm(crim~poly(medv,3),data=Boston)
summary(modele_medv)

```

Il existe une relation non linéaire significative pour le prédicteur 'Zn', 'Indus', 'nox','rm','age','rad','tax','ptratio',';stat','medv' car
on a un p-value inférieur à 0.05.

