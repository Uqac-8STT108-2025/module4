---
title: "module4"
author: "GroupeB"
date: "`r Sys.Date()`"
output: html_document
---

```{r librairies_importation, include=FALSE}
#install.packages("ISLR")
library(tidyverse)
library("ISLR") #pour charger la librairie auto
```

### Exercice1 :

* Description des hypothèses nulles :
Pour chaque variable, l'hypothèse nulle  stipule que cette qu'elle n'a aucun effet significatif sur les ventes. 
C'est à dire : 

   - TV ; H_0 : " Les dépenses en publicité télévisée n'ont pas d'effet sur les ventes."
   
   - Radio ; H_0: "Les dépenses en publicité radio n'ont pas d'effet sur les ventes."
   
   - Journaux ; H_0 : "Les dépenses en publicité dans les journaux n'ont pas d'effet sur les ventes."
   
   - Intercept ; H_0 : "La variable de référence (elle n'est pas explicitement mentionnée dans le cadre de l'exercice) 
   est non négligeable lorsque toutes les variables indépendantes sont nulles."
   
   
   
  **Interprétation des résultats** :
  
Publicité télévisée et radio : un effet significatif sur les ventes

Les valeurs p pour la TV (<0.0001) et la radio (<0.0001) sont très faibles.
Cela signifie que nous rejetons l'hypothèse nulle dans ces deux cas : il y a des preuves statistiques fortes que la publicité télévisée et la publicité radio ont un effet significatif sur les ventes.
Publicité dans les journaux : pas d'effet significatif sur les ventes

La valeur p de la publicité dans les journaux (0.8599) est très élevée.
Cela signifie que nous ne rejetons pas l'hypothèse nulle : il n'y a pas de preuve statistique que la publicité dans les journaux ait un impact sur les ventes. Autrement dit, investir dans ce type de publicité ne semble pas influencer significativement les ventes.
Intercept : significatif

La valeur p associée à l'intercept est <0.0001, donc on rejette l'hypothèse nulle.
Cela suggère qu'il existe un niveau de ventes de base même en l'absence de dépenses publicitaires.

Finalement, Les entreprises devraient privilégier la publicité télévisée et radio pour augmenter les ventes, car ces deux formes de publicité ont un impact statistiquement significatif.

### Exercice 2:

#### a) Choix de la bonne réponse : 

Nous avons $$\hat{y} = \beta _0\times + \beta _1\times GPA + \beta _2 \times QI
+ \beta _3\times Level + \beta _4\times(GPA\times QI) + \beta _5\times(GPA\times Level) + \epsilon$$


où :

- Level 1 est pour les diplômés du collège ; 

- Level 2 pour ceux du secondaire 

* Pour un diplômé du Lycée,(Level = 0), on a 

$$ \hat{y}_{Lycee} = \beta _0 +\beta _1 \times GPA + \beta _2 \times QI + \beta _4 \times GPA \times QI $$

* Pour un  diplômé du collège = universitaire : (Level = 1) on a 

$$ \hat{y}_{col} = \beta _0 +\beta _1 \times GPA + \beta _2 \times QI + \beta _3 + \beta _4 \times (GPA \times QI) + \beta _5 \times GPA $$

* On évalue maintenant la différence pour pouvoir comparer :

$$ \hat{y}_{col} - \hat{y}_{lycee} = (beta _0 +\beta _1 \times GPA + \beta _2 \times QI + \beta _3 + \beta _4 \times (GPA \times QI) + \beta _5 \times GPA) - (\beta _0 +\beta _1 \times GPA + \beta _2 \times QI + \beta _4 \times GPA \times QI)$$ 

  $$\hat{y}_{col} - \hat{y}_{lycee} = \beta _3 + \beta _5 \times GPA$$
La bonne réponse est 

iV ) Pour une valeur fixe de QI et de GPA, les diplômés universitaires gagnent en moyenne plus que les diplômés du secondaire, à condition que la GPA soit suffisamment élevée.
Mais pour que ça marche, il faut bien sûr que $\beta_3 >0$

#### b) 
Prédiction  du salaire d’un diplômé universitaire avec un QI de 110 et une moyenne cumulative de 4,0.

$$ \hat{y}_{col} = \beta _0 +\beta _1 \times GPA + \beta _2 \times QI + \beta _3 + \beta _4 \times (GPA \times QI) + \beta _5 \times GPA $$
$$ \hat{y}_{col} = \beta _0 +\beta _1 \times 4.5 + \beta _2 \times 110 + \beta _3 + \beta _4 \times (4.0 \times 110) + \beta _5 \times 4.0 $$
$$ \hat{y}_{col} = \beta _0 +4.0 \beta _1  + 110 \beta _2  + \beta _3 + 440.0 \beta _4  + 4 \beta _5  $$
Il manque les coefficients $\beta_i$ pour donner une prédiction numerique. 

#### c ) Réponse : 
Faux, ce n'est pas la valeur d'un coefficient qui détermine son interaction ou non, mais plutôt la p-value.

### Question 3 :

a ) On s'attend à ce que la RSS du modèle cubique soit plus petit que celle du modèle linéaire sur les données
d'entrainement. Par ce que le modèle cubique a plus de paramètres et capte plus de bruit, ce qui réduit significativement sa RSE.

b) regression linéaire : y_hat = a  + bX

Regression cubique : y_hat = a + bX + c X^2 + d X^3

Hypothèse nulle : c = d = 0

Si l'hypothèse nulle est rejeté (p-value significativement petit), alors le modèle cubique 
s'ajuste mieux aux donnée.

c ) (pas sur !) En toutes circonstances, la regression quadratique capte plus de bruit que la regression lineaire.
Donc la RSE est necessairement plus petite si elle est calculée sur les données test.

### Question 4 :

Question incomplete

### Question 5 :

Démontrons que dans le cas d’une régression linéaire simple, la droite des moindres carrés passe toujours par le point
moyen $G(\overline{x}, \overline{y})$.

L'équation d'une regression linéaire simple est donnée par $\hat{y} = a X + b + \epsilon$

où $$ b= \frac{\sum(x_i - \overline{x})(y_i - \overline{y})}{\sum(x_i - \overline{x})^2} $$ et 
$$b = \overline{y} - a \overline{x}$$
dès lors, $$a\overline{x} +b +\epsilon = a\overline{x} + \overline{y} - a \overline{x} \approx  \overline{y}  
 $$     cqfd.
 
### Question 6 
incomplet

## Pratique: 

### Exercice 1 

#### a) 

```{r regression-lineaire}
modele_lineaire <- lm(mpg ~ horsepower, data = Auto)
summary(modele_lineaire)
```

##### i )  
**p-value: < 2.2e-16 ** --> oui il existe une relation entre le prédicteur et la réponse ?

##### ii ) 
force de la relation entre le prédicteur et la réponse  : **R^2  0.6059** --> 60.59 % de la variabilité 
des prédictions de mpg à partir de horsepower sont expliquées par le modèle.

##### iii ) 
Signe de la relation : horsepower = -0.157845, c-a-d $\beta_ 1 <0 --> $ les deux variables évoluent en sens contraire.

##### iv ) 
mpg prévu associé à une puissance de 98 chevaux ?

```{r prediction-mpg}
predire_mpg <- function(horsepower) {
  b_0 <- 39.935861
  b_1 <- -0.157845
  return(b_0 + b_1 * horsepower)
}
predire_mpg(98)
```
La valeur prédite pour 98 cheveaux est **mpg_hat = `r round(predire_mpg(98),2)`**

#### b)
graphique entre la réponse (mpg) et le prédicteur.

```{r graphique-modele-lineaire}
plot(Auto$horsepower, Auto$mpg, 
     main = "Relation entre MPG et Horsepower", 
     xlab = "Horsepower", 
     ylab = "MPG", 
     col = "blue")  
abline(modele_lineaire, col = "black",lwd = 3)  
legend("topright", legend = "Droite de régression", col = "black", lwd = 3)

```

 v) Quels sont les intervalles de confiance et de prédiction à 95 % associés ?

#### c) 

```{r verifications-modele-lineaire }
# Générer les diagnostics du modèle
par(mfrow = c(2, 2))  
plot(modele_lineaire)  
```
commentaires :

* La distribution des résidus a une forme quadratique,, ce qui pose problème, elle ne doit pas avoir 
une répartition perceptible

* Le diagramme Quantiles-quantile a une forme lineaire --> Les résidus suivent une distribution normale



### Question 2 
Cette question implique l’utilisation d’une régression linéaire multiple sur l’ensemble de données Auto.

#### a)
Produisons une matrice de nuages de points qui inclut toutes les variables de l’ensemble de données.

```{r nuage_points}
library(ISLR)
data(Auto) 
pairs(Auto, main = "Matrice de nuages de points pour Auto")
```

#### b)
Calculons la matrice des corrélations entre les variables à l’aide de la fonction cor(). nous devions exclure la variable name qui est qualitative.

```{r matrice de correlation}
data(Auto)
#Extrait la variable "name"
Auto_numeric <- Auto[, -which(names(Auto) == "name")]

#Calculons la matrice des corrélations
cor_matrix <- cor(Auto_numeric)

print(round(cor_matrix, 2))
```
#### c) 
Utilisons la fonction lm() pour effectuer une régression linéaire multiple avec mpg comme réponse et toutes les autres variables sauf le nom comme prédicteurs. Utilisez la fonction summary() pour imprimer les résultats. Commentons le résultat. Par exemple:

 i- Existe-t-il une relation entre les prédicteurs et la réponse ?

**p-value < 2.2e-16 pour le test F, ce qui indique confirment qu'il existe une forte relation entre les prédicteurs choisis et mpg, permettant de modéliser efficacement la consommation de carburant des véhicules présents dans le jeu de données Auto.

```{r relation_predicteurs}
data(Auto)
modele_multiple <- lm(mpg ~ . - name, data = Auto)
summary(modele_multiple)
```

ii- Quels prédicteurs semblent avoir une relation statistiquement significative avec la réponse ?

Dans le modèle de régression multiple de (i), on vas sélectionner les prédicteurs qui présentent des p‑values très faibles (inférieures à 0,05):
horsepower:p = 0,21
weight: p = 2e‑16
year:p < 2e‑16
origin:p= 0,013

iii-Que suggère le coefficient de la variable « année » 

Le coefficient de la variable «année» est positif d'environ 0,75, ce qui suggère qu'en moyenne, chaque année additionnelle est associée à une augmentation d'environ 0,75 mpg. Autrement dit, les véhicules plus récents tendent à afficher une meilleure efficacité énergétique.



#### d)
Utilisons la fonction plot() pour produire des tracés de diagnostic de l’ajustement de régression linéaire. Commentez tous les problèmes que vous voyez avec l’ajustement. 

Avec le modèle_simple

```{r modele_simple}
#plaçons les graphe 2 a 2
par(mfrow = c(2, 2))
#graphe
plot(modele_multiple)
```
* Les tracés résiduels suggèrent-ils des valeurs aberrantes inhabituellement importantes ? 

D'après les tracés de diagnostic d'ajustement générés notamment le graphique "Residuals vs Fitted" et le "Residuals vs Leverage", aucun point ne se détache de manière excessive.
les résidus semblent globalement aléatoires et bien répartis, sans valeurs aberrantes inhabituelles, et le graphique d'effet de levier ne signale pas d'observations avec un effet de levier exceptionnellement élevé.

* Le graphique de l’effet de levier identifie-t-il des observations présentant un effet de levier inhabituellement élevé ?

le graphique de l'effet de levier (résidus vs levier) ne révèle aucune observation se situant en dehors des limites habituelles, Aucun point ne présente un effet de levier anormalement.

#### e)
Utilisez les symboles * et : pour ajuster les modèles de régression linéaire avec des effets d’interaction. Certaines interactions semblent-elles statistiquement significatives ?

Modèle avec interaction entre horsepower et year.

le coefficient d'interaction est négatif et statistiquement significatif car p-value < 0.05, cela suggère que l'effet négatif de horsepower sur mpg devient plus prononcé pour des véhicules plus récents.

le modele avec *

```{r régression_linéaire}
modele_interaction <- lm(mpg ~ horsepower * year, data = Auto)
summary(modele_interaction)
```
le modele avec :

```{r regression_linéaire }
modele_interaction2 <- lm(mpg ~ horsepower : year, data = Auto)
summary(modele_interaction2)
```
#### f) 
Essayons quelques transformations différentes des variables, telles que log(X), sqrt(X), X*2 Commentons vos découvertes.

Modèle original : Le modèle utilisant horsepower en l'état fournit une relation significative négative avec mpg.

```{r modèle_original}
#Modèle avec la variable originale
model_orig <- lm(mpg ~ horsepower, data = Auto)
summary(model_orig)
par(mfrow = c(2, 2))
plot(model_orig, main = "Modèle original")
```

Transformation log :L'utilisation de log(horsepower) linéarise la relation qui était autrement curviligne dans le modèle original et montre une de l'ajustement.

```{r modèle_log(X)}
# Modèle avec log(horsepower)
model_log <- lm(mpg ~ log(horsepower), data = Auto)
summary(model_log)
par(mfrow = c(2, 2))
plot(model_log, main = "Modèle log(horsepower)")
```


Transformation racine carrée :Le modèle avec sqrt(horsepower) réduire l'impact des valeurs élevées, améliore la linéarité de la relation ce qui améliore la répartition des résidus.

```{r modèle_sqrt}
# Modèle avec racine carrée de horsepower
model_sqrt <- lm(mpg ~ sqrt(horsepower), data = Auto)
summary(model_sqrt)
par(mfrow = c(2, 2))
plot(model_sqrt, main = "Modèle sqrt(horsepower)")
```

Transformation au carré :cette transformation ne semble généralement pas apporter une signification, et le terme peut même rendre l'interprétation plus complexe.

```{r modèle_carrée}
# Modèle avec horsepower au carré
model_sq <- lm(mpg ~ I(horsepower^2), data = Auto)
summary(model_sq)
par(mfrow = c(2, 2))
plot(model_sq, main = "Modèle horsepower^2")
```
### Question 3 

#### a)
Ajustons un modèle de régression multiple pour prédire les « Ventes » en utilisant « Prix », « Urbain » et « États-Unis ».

Nous allons ajuster le modèle de regression multiple utiliser la haut

```{r regression_multiple}
data(Carseats)
#Ajustons le modele utiliser la haut
modele_carseats <- lm(Sales ~ Price + Urban + US, data = Carseats)
summary(modele_carseats)
```

#### b)
Fournissons une interprétation de chaque coefficient du modèle. Soyons prudent : certaines variables du modèle sont qualitatives !

Intercept: 13.04 Ce coefficient représente la valeur prédite des ventes (Sales) pour le groupe de référence,les variables qualitatives sont codées en variables indicatrices. 
l'interception correspond aux ventes moyennes prédites pour un magasin situé dans une zone urbaine (Urban = Yes) hors des États-Unis (US = No) avec un Prix égal à 0.

Price: -0.054459 Pour chaque augmentation d'une unité du prix, en maintenant constantes toutes les autres variables, les ventes prédites diminuent en moyenne de 0.054459 unité. Cela indique que le prix a un effet négatif sur les ventes.

UrbanNo : -0.021916 Cette variable est un indicateur qui vaut 1 lorsque le magasin n'est pas situé en zone urbaine (Urban = No) et 0 lorsque le magasin est en zone urbaine (Urban = Yes). 
Le coefficient de -0.021916 suggère que, toutes choses égales par ailleurs, les magasins situés hors zone urbaine affichent des ventes prédites inférieures de 0.021916 unité par rapport aux magasins urbains. Cependant, cette différence n'est pas statistiquement significative car p-value ≈ 0,936.

USYes : 1.200573 Cette variable indicatrice vaut 1 pour les magasins situés aux États‑Unis (US = Yes) et 0 pour ceux hors des États‑Unis (US = Ne).
Le coefficient de 1.200573 indique qu'en moyenne, les magasins situés aux États-Unis affichent des ventes prédites supérieures de 1.200573 unité par rapport aux magasins non américains, toutes choses étant égales par ailleurs. Cet effet est très significatif car p-value < 2e‑1).

```{r modele_carseats}
modele_carseats <- lm(Sales ~ Price + Urban + US, data = Carseats)
summary(modele_carseats)
```
#### c)
Écrivons le modèle sous forme d’équation, en prenant soin de gérer correctement les variables qualitatives.

Sales= coef(Intercept) + coef(price)*price + coef(UrbanNo)*(Urban=no) + coef(USYes)*(US= yes).

 En application:
 
Sales = 13.04 +(-0.054459)*price + (-0.21916)*(Urban=No) + (1.200573)*(US=Yes)

NB:(Urban=No) vaut 1 si le magasin n'est pas en zone urbaine  sinon elle vaut 0 si (Urban = Yes).
-(US=YES) vaut 1 si le magasin est situé aux États-Unis  sinon elle vaut 0 si (US = NO).

#### d)
Pour lequel des prédicteurs pouvez-vous rejeter l’hypothèse nulle H₀ : Bⱼ = 0 ?

D'après le résumé du modèle sur l'ensemble de données Carseats price et us ont les plus faibles p-value, donc nous pouvons rejeter l'hypothèse nulle H₀ : Bⱼ = 0 pour les variables Price et US.

#### e)
Sur la base de notre réponse à la question précédente, ajustons un modèle plus petit qui utilise uniquement les prédicteurs pour lesquels il existe des preuves d’association avec le résultat.

Ajustons le modèle réduit en ne retenant que les prédicteurs significatifs (Price et US) :

```{r predicteurs_association}
data(Carseats)
modele_carseats_small <- lm(Sales ~ Price + US, data = Carseats)
summary(modele_carseats_small)
```
#### f)
Dans quelle mesure les modèles (a) et (e) s’ajustent-ils aux données ?

Les deux modèles s’ajustent globalement bien aux données. Par exemple, dans le modèle (a) complet qui inclut Price, Urban et US.Meme en ajustant un modèle réduit (modèle(e)) ne retenant que Price et US (car Urban n'est pas significatif), on obtient des statistiques très proches

En gros la capacité prédictive du modèle est quasiment la même que l'on utilise ou non la variable Urban, ce qui confirme que seuls Price et US apportent une information significative pour expliquer Sales.

#### g) 
À l’aide du modèle de (e), obtenons des intervalles de confiance de 95 % pour le(s) coefficient(s).

```{r intervall_confiance}
data(Carseats)
modele_carseats_small <- lm(Sales ~ Price + US, data = Carseats)
confint(modele_carseats_small, level = 0.95)
```

#### h) 
Existe-t-il des preuves de valeurs aberrantes ou d’observations à effet de levier élevé dans le modèle de (e) ?

Le graphique "Residuals vs Fitted" montre que les résidus sont bien répartis sans points extrêmes.
Le "Normal QQ Plot" indique que les résidus suivent globalement une distribution normale.

Le tracé "Residuals vs Leverage" ne met pas en évidence d'observations avec un effet de levier

Dans l'ensemble, ces diagnostics ne révèlent pas de valeurs aberrantes ni d'observations avec un effet de levier excessif dans le modèle(e).

```{r graphe observation levier}
data(Carseats)
modele_carseats_small <- lm(Sales ~ Price + US, data = Carseats)
par(mfrow = c(2, 2))
plot(modele_carseats_small)
```

### Question 4

```{r generation-alleatoire-predicteur}
  set.seed(1)
  x <- rnorm(100)
  y <- 2 * x + rnorm(100)
```

#### a) 
Regression lineaire sans intercept de y sur x

```{r regression-sans-intercept-de-y-sur-x}
modele_lin_yx <- lm(y~x+0)
summary(modele_lin_yx)

```
* Estimation du coefficient :  **beta1_hat = 1.9939**,

*erreur type : **std = 0.1065**,

* statistique t : **t = 18.73**, 

*p-value : **p < 2e-16  **

* Commentaire : Il existe une correlation positive entre x et y. La p-value est extremement faible, ce qui signifie que nous rejetons l'hypothese nulle (beta1 = 0) ==> il y a une forte correlation lineaire entre ces deux variables, et 
le coefficient estimé est significativement différent de 0.

#### b)
Regression lineaire sans intercept de x sur y

```{r regression-sans-intercept-de-x-sur-y}
modele_lin_xy <- lm(x~y+0)
summary(modele_lin_xy)
```
* Estimation du coefficient : **beta1_hat = 0.39111 **

* erreur type : **std = 0.02089**

* statistique t : **t = 18.73**

* p-value : **p <2e-16**

* Commentaire : Il existe une correlation positive entre x et y. La p-value est extremement faible, ce qui signifie que nous rejetons l'hypothese nulle (beta_1 = 0) ==> il y a une forte correlation lineaire entre ces deux variables, et 
le coefficient estimé est significativement différent de 0.

#### c) 
Les deux relations ont la même statistique t et la meme p-value. De plus, le produit des deux 
coefficient est égal à R^2

R^2= 0.7798 et   beta1_xy * beta1-yx  = `r  round(modele_lin_xy$coefficients * modele_lin_yx$coefficients,4)`

#### d) 
Question manquante

#### e)
Question manquante

#### f)
Dans R, montrez que lorsque la régression est effectuée avec un intercept la statistique t pour 
est la même pour la régression de y sur x que pour la régression de x sur y.

* Regression linéaire de y sur x avec intercept : 

```{r regression-avec-intercept-de-y-sur-x, include=FALSE}
modele_lin_yx_intercep <- lm(y~x)
model_summary_yx <- summary(modele_lin_yx_intercep)
t_stat_yx <- model_summary_yx$coefficients["x", "t value"]
cat('la statistique du test est ' , t_stat_yx )
```
La statistique du test est : **t= `r t_stat_yx`**

** Regression linéaire de x sur y avec intercept : 

```{r regression-avec-intercept-de-x-sur-y, include= FALSE}
modele_lin_xy_intercep <- lm(x~y)
model_summary_xy <- summary(modele_lin_xy_intercep)
t_stat_xy <- model_summary_yx$coefficients["x", "t value"]
cat('la statistique du test est ' , t_stat_xy )

```
La statistique du test est : **t= `r t_stat_xy`**


###Question 5

#a-Dans quelles circonstances l’estimation du coefficient pour la régression de sur est-elle la même que l’estimation du coefficient pour la régression de sur  ?

Dans une régression linéaire sans intercept,l'estimation du coefficient pour la régression de Y sur X est la même que celle de la régression de X sur Y uniquement lorsque la dispersion (la somme des carrés) de X est égale à celle de Y.

#b-Générons un exemple dans R avec observations dans lequel l’estimation du coefficient pour la régression de sur est différente de l’estimation du coefficient pour la régression de sur .

```{r exemple estimation du coefficient regression ou X diff Y}
#Exemple : Y = 2 * X (donc c = 2)
set.seed(123)
x <- rnorm(100)
y <- 2 * x

#La régression sans intercept de Y sur X
model_y_on_x <- lm(y ~ 0 + x)
coef_y_on_x <- coef(model_y_on_x)

#La régression sans intercept de X sur Y
model_x_on_y <- lm(x ~ 0 + y)
coef_x_on_y <- coef(model_x_on_y)

cat("Coefficient pour la régression de Y sur X :", round(coef_y_on_x, 3), "\n")
cat("Coefficient pour la régression de X sur Y :", round(coef_x_on_y, 3), "\n")
```

#c-Générons un exemple dans R avec observations dans lequel l’estimation du coefficient pour la régression de sur est la même que l’estimation du coefficient pour la régression de sur.

```{r exemple estimation du coefficient regression ou X,Y}
set.seed(123)
x <- rnorm(100)
y <- x  #Y est exactement égal à X

#La régression sans intercept de Y sur X
modele_Y_on_X <- lm(y ~ 0 + x)
coef_Y_on_X <- coef(modele_Y_on_X)

#La régression sans intercept de X sur Y
modele_X_on_Y <- lm(x ~ 0 + y)
coef_X_on_Y <- coef(modele_X_on_Y)

cat("Coefficient pour la régression de Y sur X :", round(coef_Y_on_X, 3), "\n")
cat("Coefficient pour la régression de X sur Y :", round(coef_X_on_Y, 3), "\n")
```
###Question 6

#a)
Pour créer un vecteur x de 100 observations tirées d'une distribution normale standard (N(0,1)) en assurant la reproductibilité, on vas utiliser
:
```{r vecteur x avec 100 observation}
set.seed(1)
x <- rnorm(100)
```

#b)
Pour créer un vecteur eps de 100 observations issues d'une distribution normale avec une moyenne nulle et une variance de 0,25 (donc un écart type de 0,5), ont vas utiliser :

```{r vecteur eps avec 100 observation }
eps <- rnorm(100, mean = 0, sd = 0.5)
```

#c)
En utilisant x et eps, générons un vecteur y selon le modèle 
Quelle est la longueur du vecteur y ? 
la longueur est 100

Quelles sont les valeurs de et dans ce modèle linéaire ?
La valeur est 2
```{r vecteur y}
y <- 2 + 2 * x + eps
length(y)
```


#d)

Le graphique montre une relation linéaire positive entre x et y, ce qui est conforme au modèle générateur x=2+2𝑥+𝜖de plus aucun comportement non linéaire ni valeurs aberrantes marquées ne semblent apparaître, ce qui confirme que la relation entre x et y est bien linéaire
```{r graphe rtion entre X et Y}
plot(x, y, main = "Nuage de points : x vs y", xlab = "x", ylab = "y", pch = 19, col = "blue")
```

#e)
Le coefficient de l'interception est estimé à environ 2,03 et celui de x à environ 1,98, ce qui est très proche des valeurs théoriques 2 et 2.
Les p‑values extrêmement faibles (p_value< 2e-16) indiquent que les deux coefficients sont très significatifs.
Le Multiple R carrée (environ 0.941) montre que le modèle explique une grande partie de la variance de y.
Ces résultats confirment que le modèle amélioré par moindres carrés reproduit correctement le modèle générateur x=2+2 x+ϵ.

```{r modèle lineaire des moindres carrée }
set.seed(1)
x <- rnorm(100)
#La variance = 0.25, écart type = 0.5
eps <- rnorm(100, mean = 0, sd = 0.5) 
y <- 2 + 2 * x + eps

#L'ajustement du modèle linéaire
modele <- lm(y ~ x)
summary(modele)
```
#f)
le nuage de points, ajoute la ligne de régression obtenue par les moindres carrés (modèle ajusté) et la vraie ligne de régression

```{r Nuage de points avec ligne des moindres carrés et vraie régression}
set.seed(1)
x <- rnorm(100)
#L'écart type = 0.5 pour une variance de 0.25
eps <- rnorm(100, mean = 0, sd = 0.5)  
y <- 2 + 2 * x + eps

#La Création du nuage de points
plot(x, y, main = "Nuage de points avec ligne des moindres carrés et vraie régression",
     xlab = "x", ylab = "y", pch = 19, col = "gray")

#L'ajustement du modèle linéaire par moindres carrés
modele <- lm(y ~ x)

#On ajout de la ligne des moindres carrés (fitted model) en bleu)
abline(modele, col = "blue", lwd = 2)

#on ajout de la vraie ligne de régression y = 2 + 2*x en rouge (trait pointillé)
abline(a = 2, b = 2, col = "red", lwd = 2, lty = 2)

#on ajout d'une légende
legend("topleft", legend = c("Ligne des moindres carrés", "Régression de la population"),
       col = c("blue", "red"), lty = c(1, 2), lwd = 2)
```
#g)

Explication:
le modèle générateur est x=2+2 x+ϵ(donc linéaire) et l'ajout du terme x² n'a pas apporter d'amélioration significative.
Or dans le modèle polynomial, le coefficient associé à I(x²) permet d'évaluer l'effet quadratique.
Si ce coefficient n'est pas statistiquement significatif (p-value > 0.05) et si le test d'ANOVA (comparaison entre le modèle linéaire et le modèle polynomial) ne montre pas une significative (p-value > 0.05), cela indique qu'il n'existe pas de preuves que le terme quadratique améliore l'ajustement du modèle.

```{r modèle de régression polynomiale}
set.seed(1)
x <- rnorm(100)
eps <- rnorm(100, mean = 0, sd = 0.5)
y <- 2 + 2 * x + eps

#Le modèle linéaire simple
model_linear <- lm(y ~ x)

#Le modèle polynomiale (avec x et x^2)
model_poly <- lm(y ~ x + I(x^2))

#Le résumés des modèles
summary(model_linear)
summary(model_poly)

#La comparaison formelle par un test d'anova
anova(model_linear, model_poly)
```
#h)
En diminuant la variance du terme d'erreur, on obtient des estimations plus précises et un meilleur ajustement global du modèle aux données, tout en confirmant que le modèle linéaire simple est bien adapté à la structure de la relation simulée.

```{r processus de génération de données pour diminuer le bruit}
#(a) Générons des données avec moins de bruit
set.seed(1)
x <- rnorm(100)                        
eps <- rnorm(100, mean = 0, sd = 0.25)   
y <- 2 + 2 * x + eps                   

#(b) Vérification de la longueur y=100
length(y)  

#(c) Nuage de points
plot(x, y, main = "Nuage de points : x vs y (moins de bruit)",
     xlab = "x", ylab = "y", pch = 19, col = "blue")

#(d) Ajustement d'un modèle linéaire simple
model_linear <- lm(y ~ x)
summary(model_linear)

#(e) Ajoutons de la droite des moindres carrés et de la vraie ligne de régression
plot(x, y, main = "x vs y : Moindres carrés vs Vraie régression (moins de bruit)",
     xlab = "x", ylab = "y", pch = 19, col = "gray")
abline(model_linear, col = "blue", lwd = 2)
abline(a = 2, b = 2, col = "red", lwd = 2, lty = 2)
legend("topleft", legend = c("Modèle ajusté", "Modèle vrai"),
       col = c("blue", "red"), lty = c(1, 2), lwd = 2)

#(f) Ajustement d'un modèle de régression polynomiale (avec x et x^2) et comparaison
model_poly <- lm(y ~ x + I(x^2))
summary(model_poly)
anova(model_linear, model_poly)
```
#i)

Le fait d'augmenter la variance du terme d'erreur rend le modèle moins précis (erreurs standards plus élevées, R² plus faible), et le nuage de points montre une plus grande dispersion. Cependant, la relation linéaire reste clairement visible, et le modèle amélioré par moindres carrés reste cohérent avec le modèle théorique x=2+2 x+ϵ.

```{r  génère des données avec plus de bruit}
#(a) Générons des données avec plus de bruit
set.seed(1)
x <- rnorm(100)                       
eps <- rnorm(100, mean = 0, sd = 1)      
y <- 2 + 2 * x + eps                  

#(b) Vérification de la longueur du vecteur y=100
length(y)  

#(c) Nuage de points entre x et y
plot(x, y, main = "Nuage de points : x vs y (plus de bruit)",
     xlab = "x", ylab = "y", pch = 19, col = "blue")

#(d) Ajustement du modèle linéaire simple et résumé
model_linear <- lm(y ~ x)
summary(model_linear)

#(e) Tracons les lignes : 
plot(x, y, main = "x vs y : Modèle ajusté vs fonction vraie (plus de bruit)",
     xlab = "x", ylab = "y", pch = 19, col = "gray")
abline(model_linear, col = "blue", lwd = 2)
abline(a = 2, b = 2, col = "red", lwd = 2, lty = 2) 
legend("topleft", legend = c("Modèle ajusté", "Modèle vrai"),
       col = c("blue", "red"), lty = c(1, 2), lwd = 2)

#(f) Ajustement du modèle de régression polynomiale (incluant x et x^2)
model_poly <- lm(y ~ x + I(x^2))
summary(model_poly)

#Comparaison entre le modèle linéaire simple et le modèle polynomial
anova(model_linear, model_poly)
```

#j)

Explication:
Dans le jeu de données avec moins de bruit , la dispersion autour de la droite de régression est plus faible. Par conséquent, les intervalles de confiance pour l'interception et le coefficient de x sont très étroits, reflétant une incertitude moindre dans les estimations.

Dans le jeu de données avec plus de bruit , la variabilité des observations autour de la droite est plus élevée, ce qui conduit à des erreurs standards plus grandes et à des intervalles de confiance plus larges. Les intervalles sont ainsi plus étendus, traduisant une incertitude accumulée dans l'estimation des coefficients.

Dans le jeu de données original , les intervalles se situent entre ceux obtenus avec moins de bruit et ceux obtenus avec plus de bruit, ce qui est cohérent avec le fait que la variance du terme d'erreur influe directement sur la précision des estimations.

Ces observations montrent clairement comment la quantité de bruit (variance de l'erreur) affecte la précision (et donc la largeur des intervalles de confiance) des coefficients estimés dans un modèle de régression linéaire.

```{r  intervalles de confiance}
set.seed(1)
#Le Jeu de données original
x <- rnorm(100)
eps_original <- rnorm(100, mean = 0, sd = 0.5)
y_original <- 2 + 2 * x + eps_original
model_original <- lm(y_original ~ x)
ci_original <- confint(model_original, level = 0.95)
print("Intervalle de confiance (jeu de données original) :")
print(round(ci_original, 3))

#Le Jeu de données avec plus de bruit
eps_bruit <- rnorm(100, mean = 0, sd = 1)
y_bruit <- 2 + 2 * x + eps_bruit
model_bruit <- lm(y_bruit ~ x)
ci_bruit <- confint(model_bruit, level = 0.95)
print("Intervalle de confiance (jeu de données avec plus de bruit) :")
print(round(ci_bruit, 3))

# Jeu de données avec moins de bruit (ε ~ N(0, 0.25²))
eps_mbruit <- rnorm(100, mean = 0, sd = 0.25)
y_mbruit <- 2 + 2 * x + eps_mbruit
model_mbruit <- lm(y_mbruit ~ x)
ci_mbruit <- confint(model_mbruit, level = 0.95)
print("Intervalle de confiance (jeu de données avec moins de bruit) :")
print(round(ci_mbruit, 3))
```

