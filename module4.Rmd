---
title: "module4"
author: "GroupeB"
date: "`r Sys.Date()`"
output: html_document
---

```{r librairies_importation, include=FALSE}
#install.packages("ISLR")
library(tidyverse)
library("ISLR") #pour charger la librairie auto
```

### Exercice1 :

* Description des hypothèses nulles :
Pour chaque variable, l'hypothèse nulle  stipule que cette qu'elle n'a aucun effet significatif sur les ventes. 
C'est à dire : 

   - TV ; H_0 : " Les dépenses en publicité télévisée n'ont pas d'effet sur les ventes."
   
   - Radio ; H_0: "Les dépenses en publicité radio n'ont pas d'effet sur les ventes."
   
   - Journaux ; H_0 : "Les dépenses en publicité dans les journaux n'ont pas d'effet sur les ventes."
   
   - Intercept ; H_0 : "La variable de référence (elle n'est pas explicitement mentionnée dans le cadre de l'exercice) 
   est non négligeable lorsque toutes les variables indépendantes sont nulles."
   
   
   
  **Interprétation des résultats** :
  
Publicité télévisée et radio : un effet significatif sur les ventes

Les valeurs p pour la TV (<0.0001) et la radio (<0.0001) sont très faibles.
Cela signifie que nous rejetons l'hypothèse nulle dans ces deux cas : il y a des preuves statistiques fortes que la publicité télévisée et la publicité radio ont un effet significatif sur les ventes.
Publicité dans les journaux : pas d'effet significatif sur les ventes

La valeur p de la publicité dans les journaux (0.8599) est très élevée.
Cela signifie que nous ne rejetons pas l'hypothèse nulle : il n'y a pas de preuve statistique que la publicité dans les journaux ait un impact sur les ventes. Autrement dit, investir dans ce type de publicité ne semble pas influencer significativement les ventes.
Intercept : significatif

La valeur p associée à l'intercept est <0.0001, donc on rejette l'hypothèse nulle.
Cela suggère qu'il existe un niveau de ventes de base même en l'absence de dépenses publicitaires.

Finalement, Les entreprises devraient privilégier la publicité télévisée et radio pour augmenter les ventes, car ces deux formes de publicité ont un impact statistiquement significatif.

### Exercice 2:

#### a) Choix de la bonne réponse : 

Nous avons $$\hat{y} = \beta _0\times + \beta _1\times GPA + \beta _2 \times QI
+ \beta _3\times Level + \beta _4\times(GPA\times QI) + \beta _5\times(GPA\times Level) + \epsilon$$


où :

- Level 1 est pour les diplômés du collège ; 

- Level 2 pour ceux du secondaire 

* Pour un diplômé du Lycée,(Level = 0), on a 

$$ \hat{y}_{Lycee} = \beta _0 +\beta _1 \times GPA + \beta _2 \times QI + \beta _4 \times GPA \times QI $$

* Pour un  diplômé du collège = universitaire : (Level = 1) on a 

$$ \hat{y}_{col} = \beta _0 +\beta _1 \times GPA + \beta _2 \times QI + \beta _3 + \beta _4 \times (GPA \times QI) + \beta _5 \times GPA $$

* On évalue maintenant la différence pour pouvoir comparer :

$$ \hat{y}_{col} - \hat{y}_{lycee} = (beta _0 +\beta _1 \times GPA + \beta _2 \times QI + \beta _3 + \beta _4 \times (GPA \times QI) + \beta _5 \times GPA) - (\beta _0 +\beta _1 \times GPA + \beta _2 \times QI + \beta _4 \times GPA \times QI)$$ 

  $$\hat{y}_{col} - \hat{y}_{lycee} = \beta _3 + \beta _5 \times GPA$$
La bonne réponse est 

iV ) Pour une valeur fixe de QI et de GPA, les diplômés universitaires gagnent en moyenne plus que les diplômés du secondaire, à condition que la GPA soit suffisamment élevée.
Mais pour que ça marche, il faut bien sûr que $\beta_3 >0$

#### b) 
Prédiction  du salaire d’un diplômé universitaire avec un QI de 110 et une moyenne cumulative de 4,0.

$$ \hat{y}_{col} = \beta _0 +\beta _1 \times GPA + \beta _2 \times QI + \beta _3 + \beta _4 \times (GPA \times QI) + \beta _5 \times GPA $$
$$ \hat{y}_{col} = \beta _0 +\beta _1 \times 4.5 + \beta _2 \times 110 + \beta _3 + \beta _4 \times (4.0 \times 110) + \beta _5 \times 4.0 $$
$$ \hat{y}_{col} = \beta _0 +4.0 \beta _1  + 110 \beta _2  + \beta _3 + 440.0 \beta _4  + 4 \beta _5  $$
Il manque les coefficients $\beta_i$ pour donner une prédiction numerique. 

#### c ) Réponse : 
Faux, ce n'est pas la valeur d'un coefficient qui détermine son interaction ou non, mais plutôt la p-value.

### Question 3 :

a ) On s'attend à ce que la RSS du modèle cubique soit plus petit que celle du modèle linéaire sur les données
d'entrainement. Par ce que le modèle cubique a plus de paramètres et capte plus de bruit, ce qui réduit significativement sa RSE.

b) regression linéaire : y_hat = a  + bX

Regression cubique : y_hat = a + bX + c X^2 + d X^3

Hypothèse nulle : c = d = 0

Si l'hypothèse nulle est rejeté (p-value significativement petit), alors le modèle cubique 
s'ajuste mieux aux donnée.

c ) (pas sur !) En toutes circonstances, la regression quadratique capte plus de bruit que la regression lineaire.
Donc la RSE est necessairement plus petite si elle est calculée sur les données test.

### Question 4 :

Question incomplete

### Question 5 :

Démontrons que dans le cas d’une régression linéaire simple, la droite des moindres carrés passe toujours par le point
moyen $G(\overline{x}, \overline{y})$.

L'équation d'une regression linéaire simple est donnée par $\hat{y} = a X + b + \epsilon$

où $$ b= \frac{\sum(x_i - \overline{x})(y_i - \overline{y})}{\sum(x_i - \overline{x})^2} $$ et 
$$b = \overline{y} - a \overline{x}$$
dès lors, $$a\overline{x} +b +\epsilon = a\overline{x} + \overline{y} - a \overline{x} \approx  \overline{y}  
 $$     cqfd.
 
### Question 6 
incomplet

## Pratique: 

### Exercice 1 

#### a) 

```{r regression-lineaire}
modele_lineaire <- lm(mpg ~ horsepower, data = Auto)
summary(modele_lineaire)
```

##### i )  
**p-value: < 2.2e-16 ** --> oui il existe une relation entre le prédicteur et la réponse ?

##### ii ) 
force de la relation entre le prédicteur et la réponse  : **R^2  0.6059** --> 60.59 % de la variabilité 
des prédictions de mpg à partir de horsepower sont expliquées par le modèle.

##### iii ) 
Signe de la relation : horsepower = -0.157845, c-a-d $\beta_ 1 <0 --> $ les deux variables évoluent en sens contraire.

##### iv ) 
mpg prévu associé à une puissance de 98 chevaux ?

```{r prediction-mpg}
predire_mpg <- function(horsepower) {
  b_0 <- 39.935861
  b_1 <- -0.157845
  return(b_0 + b_1 * horsepower)
}
predire_mpg(98)
```
La valeur prédite pour 98 cheveaux est **mpg_hat = `r round(predire_mpg(98),2)`**

###v-Quels sont les intervalles de confiance et de prédiction à 95 % associés ?

#Question 2 Cette question implique l’utilisation d’une régression linéaire multiple sur l’ensemble de données Auto.

###a-Produisons une matrice de nuages de points qui inclut toutes les variables de l’ensemble de données.
```{r}
library(ISLR) 
data(Auto) 
pairs(Auto, main = "Matrice de nuages de points pour Auto")
```

###b-Calculons la matrice des corrélations entre les variables à l’aide de la fonction cor(). nous devions exclure la variable name qui est qualitative.

```{r}
data(Auto)
#Extrait la variable "name"
Auto_numeric <- Auto[, -which(names(Auto) == "name")]

#Calculons la matrice des corrélations
cor_matrix <- cor(Auto_numeric)

print(round(cor_matrix, 2))
```
#c-Utilisons la fonction lm() pour effectuer une régression linéaire multiple avec mpg comme réponse et toutes les autres variables sauf le nom comme prédicteurs. Utilisez la fonction summary() pour imprimer les résultats. Commentons le résultat. Par exemple:

###i-Existe-t-il une relation entre les prédicteurs et la réponse ?

**p-value < 2.2e-16 pour le test F, ce qui indique confirment qu'il existe une forte relation entre les prédicteurs choisis et mpg, permettant de modéliser efficacement la consommation de carburant des véhicules présents dans le jeu de données Auto.

```{r}
data(Auto)
modele_multiple <- lm(mpg ~ . - name, data = Auto)
summary(modele_multiple)
```

##ii-Quels prédicteurs semblent avoir une relation statistiquement significative avec la réponse?

Dans le modèle de régression multiple de (i), on vas sélectionner les prédicteurs qui présentent des p‑values très faibles (inférieures à 0,05):
horsepower:p = 0,21
weight: p = 2e‑16
year:p < 2e‑16
origin:p= 0,013

###iii-Que suggère le coefficient de la variable « année » 

Le coefficient de la variable «année» est positif d'environ 0,75, ce qui suggère qu'en moyenne, chaque année additionnelle est associée à une augmentation d'environ 0,75 mpg. Autrement dit, les véhicules plus récents tendent à afficher une meilleure efficacité énergétique.


#d-Utilisons la fonction plot() pour produire des tracés de diagnostic de l’ajustement de régression linéaire. Commentez tous les problèmes que vous voyez avec l’ajustement. 

Avec le modèle_simple

```{r}
#plaçons les graphe 2 a 2
par(mfrow = c(2, 2))
#graphe
plot(modele_multiple)
```

###Les tracés résiduels suggèrent-ils des valeurs aberrantes inhabituellement importantes ? 

D'après les tracés de diagnostic d'ajustement générés notamment le graphique "Residuals vs Fitted" et le "Residuals vs Leverage", aucun point ne se détache de manière excessive.
les résidus semblent globalement aléatoires et bien répartis, sans valeurs aberrantes inhabituelles, et le graphique d'effet de levier ne signale pas d'observations avec un effet de levier exceptionnellement élevé.

###Le graphique de l’effet de levier identifie-t-il des observations présentant un effet de levier inhabituellement élevé ?

le graphique de l'effet de levier (résidus vs levier) ne révèle aucune observation se situant en dehors des limites habituelles, Aucun point ne présente un effet de levier anormalement.

###e-Utilisez les symboles * et : pour ajuster les modèles de régression linéaire avec des effets d’interaction. Certaines interactions semblent-elles statistiquement significatives ?

Modèle avec interaction entre horsepower et year.

le coefficient d'interaction est négatif et statistiquement significatif car p-value < 0.05, cela suggère que l'effet négatif de horsepower sur mpg devient plus prononcé pour des véhicules plus récents.

le modele avec *

```{r}
modele_interaction <- lm(mpg ~ horsepower * year, data = Auto)
summary(modele_interaction)
```
le modele avec :

```{r}
modele_interaction2 <- lm(mpg ~ horsepower : year, data = Auto)
summary(modele_interaction2)
```
###f-Essayons quelques transformations différentes des variables, telles que log(X), sqrt(X), X*2 Commentons vos découvertes.

Modèle original : Le modèle utilisant horsepower en l'état fournit une relation significative négative avec mpg.

```{r}
#Modèle avec la variable originale
model_orig <- lm(mpg ~ horsepower, data = Auto)
summary(model_orig)
par(mfrow = c(2, 2))
plot(model_orig, main = "Modèle original")
```

Transformation log :L'utilisation de log(horsepower) linéarise la relation qui était autrement curviligne dans le modèle original et montre une de l'ajustement.

```{r}
# Modèle avec log(horsepower)
model_log <- lm(mpg ~ log(horsepower), data = Auto)
summary(model_log)
par(mfrow = c(2, 2))
plot(model_log, main = "Modèle log(horsepower)")
```


Transformation racine carrée :Le modèle avec sqrt(horsepower) réduire l'impact des valeurs élevées, améliore la linéarité de la relation ce qui améliore la répartition des résidus.

```{r}
# Modèle avec racine carrée de horsepower
model_sqrt <- lm(mpg ~ sqrt(horsepower), data = Auto)
summary(model_sqrt)
par(mfrow = c(2, 2))
plot(model_sqrt, main = "Modèle sqrt(horsepower)")
```

Transformation au carré :cette transformation ne semble généralement pas apporter une signification, et le terme peut même rendre l'interprétation plus complexe.

```{r}
# Modèle avec horsepower au carré
model_sq <- lm(mpg ~ I(horsepower^2), data = Auto)
summary(model_sq)
par(mfrow = c(2, 2))
plot(model_sq, main = "Modèle horsepower^2")
```
###Question 3 Cette question utilise l’ensemble de données Carseats de la librairie ISLR2.

##"a-Ajustons un modèle de régression multiple pour prédire les « Ventes » en utilisant « Prix », « Urbain » et « États-Unis ».

Nous allons ajuster le modèle de regression multiple utiliser la haut

```{r}
data(Carseats)
#Ajustons le modele utiliser la haut
modele_carseats <- lm(Sales ~ Price + Urban + US, data = Carseats)
summary(modele_carseats)
```

###b-Fournissons une interprétation de chaque coefficient du modèle. Soyons prudent : certaines variables du modèle sont qualitatives !

Intercept: 13.04 Ce coefficient représente la valeur prédite des ventes (Sales) pour le groupe de référence,les variables qualitatives sont codées en variables indicatrices. 
l'interception correspond aux ventes moyennes prédites pour un magasin situé dans une zone urbaine (Urban = Yes) hors des États-Unis (US = No) avec un Prix égal à 0.

Price: -0.054459 Pour chaque augmentation d'une unité du prix, en maintenant constantes toutes les autres variables, les ventes prédites diminuent en moyenne de 0.054459 unité. Cela indique que le prix a un effet négatif sur les ventes.

UrbanNo : -0.021916 Cette variable est un indicateur qui vaut 1 lorsque le magasin n'est pas situé en zone urbaine (Urban = No) et 0 lorsque le magasin est en zone urbaine (Urban = Yes). 
Le coefficient de -0.021916 suggère que, toutes choses égales par ailleurs, les magasins situés hors zone urbaine affichent des ventes prédites inférieures de 0.021916 unité par rapport aux magasins urbains. Cependant, cette différence n'est pas statistiquement significative car p-value ≈ 0,936.

USYes : 1.200573 Cette variable indicatrice vaut 1 pour les magasins situés aux États‑Unis (US = Yes) et 0 pour ceux hors des États‑Unis (US = Ne).
Le coefficient de 1.200573 indique qu'en moyenne, les magasins situés aux États-Unis affichent des ventes prédites supérieures de 1.200573 unité par rapport aux magasins non américains, toutes choses étant égales par ailleurs. Cet effet est très significatif car p-value < 2e‑1).

```{r}
modele_carseats <- lm(Sales ~ Price + Urban + US, data = Carseats)
summary(modele_carseats)
```
###c-Écrivons le modèle sous forme d’équation, en prenant soin de gérer correctement les variables qualitatives.

Sales= coef(Intercept) + coef(price)*price + coef(UrbanNo)*(Urban=no) + coef(USYes)*(US= yes).

 En application:
 
Sales = 13.04 +(-0.054459)*price + (-0.21916)*(Urban=No) + (1.200573)*(US=Yes)

NB:(Urban=No) vaut 1 si le magasin n'est pas en zone urbaine  sinon elle vaut 0 si (Urban = Yes).
-(US=YES) vaut 1 si le magasin est situé aux États-Unis  sinon elle vaut 0 si (US = NO).

###d-Pour lequel des prédicteurs pouvez-vous rejeter l’hypothèse nulle H₀ : Bⱼ = 0 ?

D'après le résumé du modèle sur l'ensemble de données Carseats price et us ont les plus faibles p-value, donc nous pouvons rejeter l'hypothèse nulle H₀ : Bⱼ = 0 pour les variables Price et US.

###e-Sur la base de notre réponse à la question précédente, ajustons un modèle plus petit qui utilise uniquement les prédicteurs pour lesquels il existe des preuves d’association avec le résultat.

Ajustons le modèle réduit en ne retenant que les prédicteurs significatifs (Price et US) :

```{r}
data(Carseats)
modele_carseats_small <- lm(Sales ~ Price + US, data = Carseats)
summary(modele_carseats_small)
```
###f-Dans quelle mesure les modèles (a) et (e) s’ajustent-ils aux données ?

Les deux modèles s’ajustent globalement bien aux données. Par exemple, dans le modèle (a) complet qui inclut Price, Urban et US.Meme en ajustant un modèle réduit (modèle(e)) ne retenant que Price et US (car Urban n'est pas significatif), on obtient des statistiques très proches

En gros la capacité prédictive du modèle est quasiment la même que l'on utilise ou non la variable Urban, ce qui confirme que seuls Price et US apportent une information significative pour expliquer Sales.

###g-À l’aide du modèle de (e), obtenons des intervalles de confiance de 95 % pour le(s) coefficient(s).

```{r}
data(Carseats)
modele_carseats_small <- lm(Sales ~ Price + US, data = Carseats)
confint(modele_carseats_small, level = 0.95)
```
###h-Existe-t-il des preuves de valeurs aberrantes ou d’observations à effet de levier élevé dans le modèle de (e) ?

Le graphique "Residuals vs Fitted" montre que les résidus sont bien répartis sans points extrêmes.
Le "Normal QQ Plot" indique que les résidus suivent globalement une distribution normale.

Le tracé "Residuals vs Leverage" ne met pas en évidence d'observations avec un effet de levier

Dans l'ensemble, ces diagnostics ne révèlent pas de valeurs aberrantes ni d'observations avec un effet de levier excessif dans le modèle(e).

```{r}
data(Carseats)
modele_carseats_small <- lm(Sales ~ Price + US, data = Carseats)
par(mfrow = c(2, 2))
plot(modele_carseats_small)
```

